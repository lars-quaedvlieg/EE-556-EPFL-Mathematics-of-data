{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Homework 1\n",
    "\n",
    "#### EE-556 Mathematics of Data - Fall 2022"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If you worked with somebody else on this Homework, fill in their names here:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Logistic Regression - 10 Points\n",
    "\n",
    "\n",
    "We consider a binary classification task that we will model using logistic regression. Your goal will be to find a classifier using first-order methods and accelerated gradient descent methods. The first part will consist of more theoretical questions, and the second and third part will ask you to implement these methods.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Logistic regression is a classic approach to _binary classification_. Before we dive in, let us first define the standard logistic function $\\sigma$ on which most of what follows is built:\n",
    "\n",
    "\\begin{equation*}\n",
    "    \\sigma : x \\mapsto \\frac{1}{1 + \\exp{(-x)}}.\n",
    "\\end{equation*}\n",
    "\n",
    "In logistic regression, we model the _conditional probability_ of observing a class label $b$ given a set of features $\\mathbf{a}$. More formally, if we observe $n$ independent samples\n",
    "\n",
    "\\begin{equation*}\n",
    "    \\{(\\mathbf{a}_i,b_i)\\}_{i=1}^n,\n",
    "\\end{equation*}\n",
    "\n",
    "where $\\mathbf{a}_i\\in\\mathbb{R}^p$ and $b_i\\in\\{0, 1\\}$ is the class label, we _assume_ that $b_i$ given $\\mathbf{a}_i$ is a Bernouilli random variable with parameter $\\sigma(\\mathbf{a}_i^T\\mathbf{x}^\\natural)$, for some unknown $\\mathbf{x}^\\natural \\in \\mathbb{R}^p$. In other words, we assume that there exists an $\\mathbf{x}^\\natural \\in \\mathbb{R}^p$ such that\n",
    "\n",
    "\\begin{equation*}\n",
    "    \\mathbb{P}(b_i = 1 \\mid \\mathbf{a}_i) = \\sigma(\\mathbf{a}_i^T\\mathbf{x}^\\natural) \\quad \\text{ and } \\quad \\mathbb{P}(b_i = 0 \\mid \\mathbf{a}_i) = 1 - \\sigma(\\mathbf{a}_i^T\\mathbf{x}^\\natural)=  \\sigma( - \\mathbf{a}_i^T\\mathbf{x}^\\natural).\n",
    "\\end{equation*}\n",
    "\n",
    "This is our statistical model. It can be written in a more compact form as follows,\n",
    "\n",
    "\\begin{equation*}\n",
    "    \\mathbb{P}(b_i = j \\mid \\mathbf{a}_i) = \\sigma(\\mathbf{a}_i^T\\mathbf{x}^\\natural)^j\\sigma(-\\mathbf{a}_i^T\\mathbf{x}^\\natural)^{1-j}, \\quad j \\in \\{0, 1\\}.\n",
    "\\end{equation*}\n",
    "\n",
    "Our goal now is to determine the unknown $\\mathbf{x}^\\natural$ by constructing an estimator.\n",
    "\n",
    "We are provided with a set of $n$ independent observations, we can write down the negative log-likelihood $f$ as follows:\n",
    "\n",
    "\\begin{equation*}\n",
    "\t\\begin{aligned}\n",
    "\t\tf(\\mathbf{x}) = -\\log(\\mathbb{P}(b_1, \\dots, b_n | a_1, \\dots, a_n)) & = -\\log \\left( \\prod_{i=1}^{n} \\mathbb{P}(b_i|a_i) \\right) \\quad \\quad \\text{(by independence)}\\\\\n",
    "\t\t & =  \\sum_{i=1}^n - \\log(\\sigma(\\mathbf{a}_i^T\\mathbf{x}^\\natural)^{b_i}\\sigma(-\\mathbf{a}_i^T\\mathbf{x}^\\natural)^{1-b_i}) \\\\\n",
    "\t\t & = \\sum_{i=1}^n  b_i \\log(1 + \\exp(- \\mathbf{a}_i^T\\mathbf{x})) + (1-b_i)\\log(1 + \\exp(\\mathbf{a}_i^T\\mathbf{x})).\n",
    "\t\\end{aligned}\n",
    "\\end{equation*}\n",
    "\n",
    "\n",
    "__(a)__ (1 point) Show that the function $u \\mapsto \\log(1 + \\exp(u))$ is convex. Deduce that $f$ is convex."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You have just established that the negative log-likelihood is a convex function. So in principle, any local minimum of the maximum likelihood estimator, which is defined as\n",
    "\n",
    "\\begin{equation*}\n",
    "    \\mathbf{x}^\\star_{ML} = \\arg\\min_{\\mathbf{x} \\in \\mathbb{R}^p} f(\\mathbf{x}),\n",
    "\\end{equation*}\n",
    "\n",
    "is a global minimum so it can serve as an estimator of $\\mathbf{x}^\\natural$. But, does the minimum always exist? We will ponder this question in the following three points.\n",
    "\n",
    "__(b)__ (1 point) Explain the difference between infima and minima.  Give an example of a convex function, defined over $\\mathbb{R}$, that does not attain its infimum. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__(c)__ (1 points) Let us assume that there exists $\\mathbf{x}_0 \\in \\mathbb{R}^p$ such that \n",
    "\n",
    "\\begin{equation*}\n",
    "    \\forall i\\in\\{1, \\dots, n\\}, \\quad \\quad \\text{ if } b_i = 1, \\text{ then } \\mathbf{a}_i^T\\mathbf{x}_0 > 0, \\text{ and if } b_i = 0 \\text{ then } \\mathbf{a}_i^T\\mathbf{x}_0 < 0.\n",
    "\\end{equation*}\n",
    "\n",
    "This is called _complete separation_ in the literature. Can you think of a geometric reason why this name is appropriate? Think of a 2D example where this can happen (i.e $p=2$) and describe why _complete separation_ is an appropriate name.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "From this, you should see that it is likely that our data satisfies the complete separation assumption. Unfortunately, as you will show in the following question, this can become an obstacle."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__(d)__ (2 points) In a _complete separation_ setting, i.e, there exists $\\mathbf{x}_0$ such that \n",
    "\n",
    "\\begin{equation*}\n",
    "    \\forall i\\in\\{1, \\dots, n\\}, \\quad \\quad \\text{ if } b_i = 1, \\text{ then } \\mathbf{a}_i^T\\mathbf{x}_0 > 0, \\text{ and if } b_i = 0 \\text{ then } \\mathbf{a}_i^T\\mathbf{x}_0 < 0.\n",
    "\\end{equation*}\n",
    "\n",
    "prove that the function $f$ does not attain its minimum. \n",
    "\n",
    "__Hint__: If the function did have a minimum, would it be above, below or equal to zero? Then think of how $f(2 \\mathbf{x}_0)$ compares with $f(\\mathbf{x}_0)$, how about $f(\\alpha \\mathbf{x}_0)$ for $\\alpha \\rightarrow + \\infty$ ? "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As you have just shown convex functions do not always attain their infimum. So it is possible for the maximum-likelihood estimator $\\mathbf{x}^\\star_{ML}$ to not exist. We will resolve this issue by adding a regularizer. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In what follows, we consider the function\n",
    "\n",
    "\\begin{equation*}\n",
    "\tf_\\mu(\\mathbf{x}) = f(\\mathbf{x}) + \\frac{\\mu}{2}\\|\\mathbf{x}\\|_2^2\n",
    "\\end{equation*}\n",
    "with $\\mu> 0$.\n",
    "\n",
    "__(e)__ (1 points) Show that the gradient of $f_\\mu$ can be expressed as \n",
    "\\begin{equation}\n",
    "\t\\nabla f_\\mu(\\mathbf{x}) = \\sum_{i=1}^n (\\sigma(\\mathbf{a}_i^T\\mathbf{x}) - b_i)\\mathbf{a}_i + \\mu \\mathbf{x}.\n",
    "\\tag{1}\\label{gradient}\n",
    "\\end{equation}\n",
    "__Hint__: Lecture 3 shows you how to proceed with this question."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__(f)__ (1 points) Show that the Hessian of $f_\\mu$ can be expressed as \n",
    "\\begin{equation}\n",
    "\t\\nabla^2 f_\\mu(\\mathbf{x}) = \\sum_{i=1}^{n} \\sigma(\\mathbf{a}_i^T\\mathbf{x})\\sigma(- \\mathbf{a}_i^T\\mathbf{x})\\mathbf{a}_i\\mathbf{a}_i^T + \\mu \\mathbf{I}.\n",
    "\\tag{2}\\label{eq:hessian}\n",
    "\\end{equation}\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It is convenient to observe that we can write the Hessian in a more compact form by defining the matrix\n",
    "\\begin{equation}\n",
    "\t\\mathbf{A} = \\begin{bmatrix}\n",
    "        \\leftarrow &  \\mathbf{a}_1^T & \\rightarrow \\\\\n",
    "        \\leftarrow &  \\mathbf{a}_2^T & \\rightarrow \\\\\n",
    "         &  \\ldots &  \\\\\n",
    "        \\leftarrow &  \\mathbf{a}_n^T & \\rightarrow \\\\\n",
    "  \\end{bmatrix}.\n",
    "\\end{equation}\n",
    "It is easy to see that we have\n",
    "\\begin{equation}\n",
    "\t\\nabla^2 f_\\mu(\\mathbf{x}) =  \\mathbf{A}^T \\text{Diag}\\left( \\sigma(\\mathbf{a}_i^T\\mathbf{x})\\sigma(- \\mathbf{a}_i^T\\mathbf{x}) \\right)\\mathbf{A}+ \\mu \\mathbf{I}.\n",
    "\\end{equation}\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__(g)__ (1 point) Show that $f_\\mu$ is $\\mu$-strongly convex. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__(h)__ (1 point) Is it possible for a strongly convex function, defined over $\\mathbb{R}^p$, to not attain its minimum ? <a name=\"cite_ref-1\"></a>[<sup>[1]</sup>](#cite_note-1) Justify your reasoning."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will now show that $f_\\mu$ is smooth, i.e, $\\nabla f_\\mu$ is L-Lipschitz with respect to the Euclidean norm, with \n",
    "\\begin{equation}\n",
    "\tL = \\|A\\|^2_F + \\mu \\text{, where }\\|\\cdot\\|_F\\text{ denotes the Frobenius norm. }\n",
    "\\end{equation}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "(1 point for all three questions)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__(i-1)__ Show that $\\lambda_{\\max}(\\mathbf{a}_i\\mathbf{a}_i^T) = \\left\\| \\mathbf{a}_i\\right\\|_2^2$, where $\\lambda_{\\max}(\\cdot)$ denotes the largest eigenvalue."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__(i-2)__ Using [2](#mjx-eqn-eq2), show that $\\lambda_{\\max}(\\nabla^2 f_\\mu(\\mathbf{x})) \\leq \\sum_{i=1}^{n} \\|\\mathbf{a}_i\\|_2^2 + \\mu$. \n",
    "\n",
    "__Hint__: Recall that $\\lambda_{\\max}(\\cdot)$ verifies the triangle inequality."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__(i-3__ Conclude that $f_\\mu$ is $L$-smooth for $L = \\|A\\|_F^2 + \\mu$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "From your work in this section, you have shown that the maximum likelihood estimator for logistic regression might not exist, but it can be guaranteed to exist by adding a $\\|\\cdot\\|_2^2$ regularizer. Consequently, the estimator for $\\mathbf{x}^\\natural$ we will use will be the solution of the smooth strongly convex problem,\n",
    "\\begin{equation}\n",
    "\t\\mathbf{x}^\\star=\\arg\\min_{\\mathbf{x} \\in \\mathbb{R}^p} f(\\mathbf{x}) + \\frac{\\mu}{2}\\|\\mathbf{x}\\|_2^2.\n",
    "\\tag{3}\\label{eq:log_problem}\n",
    "\\end{equation}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a name=\"cite_note-1\"></a>1. [^](#cite_ref-1) TA's will give you candy if you provide a complete proof."
   ]
  }
 ],
 "metadata": {
  "jupytext": {
   "custom_cell_magics": "kql"
  },
  "kernelspec": {
   "display_name": "Python",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
