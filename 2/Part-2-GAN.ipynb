{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from IPython.display import Image\n",
    "from lib.utils import *"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Generative Adversarial Networks - 50 points\n",
    "\n",
    "Please review Lectures 10-11 for the notation and the setup in this exercise.\n",
    "\n",
    "As a basic example for the implementation of GANs, we will try to learn the density of a 2D mixture of Gaussian (MoG) distribution from empirical samples using the Wasserstein GAN (WGAN) framework. MoG distributions are multi-modal, and allow to us visualize multiple \"modes\" of the distribution. \n",
    "\n",
    "Both your generator neural network $h_\\mathbf{x}$ and dual variable neural network $\\mathtt{d}_\\mathbf{y}$ are defined as two hidden layer networks:\n",
    "\n",
    "\\begin{equation}\n",
    "    \\mathcal{H}:=\\{h: h_{\\mathbf{x}}(\\omega)=X_3\\texttt{relu}(X_2\\texttt{relu}(X_1 \\omega + x_1)+ x_2) + x_3\\},\n",
    "    \\qquad \\mathcal{D}:= \\{\\mathtt{d}: \\mathtt{d}_{\\mathbf{x}}({\\bf a})=\n",
    "    Y_3\\texttt{relu}(Y_2\\texttt{relu}(Y_1 {\\bf a} + y_1)+ y_2)+y_3\\},\n",
    "\\end{equation}\n",
    "\n",
    "where $\\mathbf{x} =[x_1;x_2;{X_1};{X_2}; X_3]$ are the \"generator\" parameters, and $\\mathbf{y} =[y_1;y_2;{Y_1};{Y_2}; Y_3] $ are the\n",
    "\"dual\" or the \"discriminator\" parameters. The dimensions of these parameters will be apparent from the context as well as the base code provided along with the homework. \n",
    "\n",
    "In the following cells, you will implement the two neural networks as well as the spectral normalization ([Myato 2018](https://arxiv.org/abs/1802.05957)) method that enforces a Lipschitz constraint."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**(1)** (3 points) Implement a two hidden layer MLP with ReLU below for the generator. See `torch.nn.Sequential` docs for a start."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Generator(nn.Module):\n",
    "\n",
    "    def __init__(self, noise_dim=2, output_dim=2, hidden_dim=100):\n",
    "        super().__init__()\n",
    "        \n",
    "        self.inner = #Fill\n",
    "\n",
    "    def forward(self, z):\n",
    "        return self.inner(z)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**(2)** (20 points) Implement a two hidden layer MLP with ReLU below for the Discriminator and implement the spectral normalization method.\n",
    "\n",
    "**Remark:** The `spectral_normalization` does not need to return a value. It should modify the parameters $\\mathbf{y}$ of the dual network $\\mathtt{d}_{\\mathbf{y}}$ in place. Remember not to track gradients in those operations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DualVariable(nn.Module):\n",
    "\n",
    "    def __init__(self, input_dim=2, hidden_dim=100, c=1e-2):\n",
    "        super().__init__()\n",
    "        self.c = c\n",
    "        \n",
    "        self.inner = #Fill\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.inner(x)\n",
    "\n",
    "    def enforce_lipschitz(self):\n",
    "        self.spectral_normalisation()\n",
    "\n",
    "    def spectral_normalisation(self):\n",
    "        \"\"\"\n",
    "        Perform spectral normalisation, forcing the singular value of the weights to be upper bounded by 1.\n",
    "        \"\"\"\n",
    "        with torch.no_grad():\n",
    "            #Fill"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**(3)** (2 points) Spectral Normalization applied to a matrix outputs a matrix whose largest singular value is upper bounded by one. Is Spectral Normalization a projection (in the $\\ell_2$ sense) ? Think of a diagonal square matrix and try figure out what spectral normalization does and compare that to what a projection onto the set of matrices with largest singular value less than 1 would do."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**(3)** (1 point) Implement a stochastic estimate of the objective function of the minimax game (in the cell below):\n",
    "\n",
    "$$\n",
    "\\min_{\\mathbf{x} \\in \\mathcal{X}} \\max_{\\mathbf{y} \\in \\mathcal{Y}} \n",
    "\\mathbb{E}[\\mathtt{d}_{\\mathbf{y}}({\\bf a})] - \\mathbb{E}[\\mathtt{d}_{\\mathbf{y}}(h_{\\mathbf{x}}(\\omega))] = \n",
    "\\min_{h \\in \\mathcal{H}} \\max_{\\mathtt{d} \\in \\mathcal{F}}\n",
    "\\mathbb{E}[\\mathtt{d}({\\bf a})] - \\mathbb{E}[\\mathtt{d}(h(\\omega))],\n",
    "$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def objective(d, g, data_sample, noise_sample):\n",
    "    W1 = #Fill\n",
    "    return W1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**(4)** (20 points) Implement an alternating gradient ascent/descent update, training the generator 1 time for every 5 dual updates. More specifically, you will implement the conceptual algorithm below using the parameters of the neural networks\n",
    "\n",
    "$$\n",
    "\\begin{aligned}\n",
    "\\mathtt{d}^{k+1} &= \\text{EnforceLipschitz} (\\mathtt{d}^k + \\gamma \\text{SG}_\\mathtt{d}(\\mathtt{d}^k, h^kk)), && \\text{(if $k$ mod $5 \\neq 0$) }\\\\\n",
    "h^{k+1} &= h^{k} - \\gamma \\text{SG}_h(\\mathtt{d}^{k+1}, h^k) && \\text{(if $k$ mod $5 =0$)},\n",
    "\\end{aligned}\n",
    "$$\n",
    "\n",
    "where $\\text{SG}$ is the stochastic gradient oracle. To perform the optimization, you have two `Pytorch` optimizers `d_optim` and `g_optim`, which have a `.step()` method that updates the discriminator and generator parameters respectively.\n",
    "\n",
    "Use the objective function you have just written. The iteration count is held by the `step_k` argument. The agument `d_ratio` defines how many more times we train the discriminator that train the generator. \n",
    "\n",
    "For later, to display the progress, write the function so that it returns the value of the objective function.\n",
    "\n",
    "**Hints**: Don't forget that the generator seeks to minimize and the discriminator seeks to maximize. Pytorch optimizers step in the _negative_ gradient direction, keep that in mind."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def alternating_update(step_k, d, g, d_optim, g_optim, noise_samples, real_samples, d_ratio=5):\n",
    "    if step_k % d_ratio==0:\n",
    "        d.eval()\n",
    "        g.train()\n",
    "        #Fill\n",
    "    else:\n",
    "        d.train()\n",
    "        g.eval()\n",
    "        #Fill\n",
    "\n",
    "    return l"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "With all the ingredients in hand, we can train our GAN.\n",
    "\n",
    "The following cell defines the two networks."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "hidden_dim = 30\n",
    "d = DualVariable(input_dim=2, hidden_dim=hidden_dim, c=1.0)\n",
    "g = Generator(noise_dim=2, output_dim=2, hidden_dim=hidden_dim)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**(5)** (2 points) Define an optimizer for each of the networks. We recommend choosing `Adam` with betas $(0.0, 0.9)$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "d_optim = #Fill\n",
    "g_optim = #Fill"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The following cell runs the training loop for 2000 iterations, this might take a few minutes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train(d, g, d_optim, g_optim, alternating_update, n_iter=2000)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Once the training is complete, you can visualize the training process in the following gif. (If you encounter problems seeing the gif, open it in a separate tab, it can be found in the same directory as this notebook)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Image(\"movie.gif\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**(6)** (2 points) Briefly comment on what you observe. Play with the network sizes, the step sizes, the number of iterations, all while keeping in mind the difficulties of min-max optimization you've analyzed in the previous notebook."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "jupytext": {
   "custom_cell_magics": "kql",
   "formats": "ipynb,py:percent,md"
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
